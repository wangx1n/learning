# java

## Reetrantlock是如何实现可重入的

https://blog.csdn.net/weixin_32281549/article/details/112350360

1. 通过cas操作置锁的state字段为1
2. 再次获取state时发现为1，置1失败
3. 置1失败后判断是否是当前线程获取的锁，是的话state+1
4. 释放时state为0时才能彻底释放锁

## ConcurrentHashMap

### 1. 数据结构

### 2. 参数

### 3. 源码

### 4. 扩容流程

### 5. CAS + synchronized

### 6. volatile



## 为什么要从永久代转变为元空间

　	1、字符串存在永久代中，容易出现性能问题和内存溢出。

　　2、类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。

　　3、永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。

　　4、Oracle 可能会将HotSpot 与 JRockit 合二为一。



## java内存区域

https://github.com/Snailclimb/JavaGuide/blob/3965c02cc0f294b0bd3580df4868d5e396959e2e/Java%E7%9B%B8%E5%85%B3/%E5%8F%AF%E8%83%BD%E6%98%AF%E6%8A%8AJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E8%AE%B2%E7%9A%84%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0.md#22-java-%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88



## GC ROOT 在哪里

- 所有Java线程当前活跃的栈帧里指向GC堆里的对象的引用；换句话说，当前所有正在被调用的方法的引用类型的参数/局部变量/临时值。
- VM的一些静态数据结构里指向GC堆里的对象的引用，例如说HotSpot VM里的Universe里有很多这样的引用。



## 标记清除、标记复制、标记整理

https://blog.csdn.net/weixin_38168947/article/details/109066221

##  指针碰撞、空闲列表

https://blog.csdn.net/hyman_c/article/details/103051359

内存规整用指针碰撞、不规整用空闲列表



# kafka

## kafka完全指南

https://www.cnblogs.com/sujing/p/10960832.html

### kafka是如何实现高性能的

https://zhuanlan.zhihu.com/p/106033054

1. 多分区

2. 顺序读写磁盘

3. 充分利用page cache（kafka是如何充分利用page cache的）https://blog.csdn.net/gx11251143/article/details/107620259

   1. pageCache是什么 平衡内存与磁盘速度的媒介

   2. kafka是怎么使用pageCache的 kafka写直接写pageCache, 最好读写速度匹配，做到空中接力，否则读磁盘影响线上

   3. pageCache参数 dirty_expire_centisecs 写入磁盘的速度 默认半分钟 pageCache被标记为dirty的话，超过这个时间写入磁盘

      dirty_background_ratio pageCache的总大小占内存空间的百分比，默认10%，超过异步写入磁盘

      dirty_ratio dirty page的总大小占总内存量的比例 默认20%, 超过这个比例，阻塞所有write，强制每个进程将自己的文件写入磁盘

4. 零拷贝（什么是零拷贝）https://www.cnblogs.com/zz-ksw/p/12801632.html

   1. mmap
   2. sendfile

   https://zhuanlan.zhihu.com/p/63626191 这篇将0拷贝讲的好一些



## 为什么要用kafka

1. 缓冲、削峰 ： 上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。
2. 解耦 ： 项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。
3. 冗余 ： 可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。
4. 健壮性 ： 消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。
5. 异步通信 ： 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。



## 如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？

1.  partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。
2. 如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。
3. 如果既没指定partition，又没有设置key，则会轮询选出一个partition。



## Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么

ISR:In-Sync Replicas 副本同步队列
AR:Assigned Replicas 所有副本
**ISR是由leader维护**，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。**AR=ISR+OSR**。



## kafka中的broker 是干什么的

broker 是消息的代理，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉取指定Topic的消息，然后进行业务处理，broker在中间起到一个代理保存消息的中转站。



## kafka中的 zookeeper 起到什么作用，可以不用zookeeper么

zookeeper 是一个分布式的协调组件，早期版本的kafka用zk做meta信息存储，consumer的消费状态，group的管理以及 offset的值。考虑到zk本身的一些因素以及整个架构较大概率存在单点问题，新版本中逐渐弱化了zookeeper的作用。新的consumer使用了kafka内部的**group coordination**协议，也减少了对zookeeper的依赖，

但是broker依然依赖于ZK，zookeeper 在kafka中还用来选举controller 和 检测broker是否存活等等。



## kafka follower如何与leader同步数据

Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果leader挂掉，会丢失数据，kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。



## 什么情况下一个 broker 会从 isr中踢出去

leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 ，如果一个follower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 。



## 为什么Kafka不支持读写分离？

在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种主写主读的生产消费模型。

Kafka 并不支持主写从读，因为主写从读有 2 个很明 显的缺点:

(1)**数据一致性问题**。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。

(2)**延时问题**。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。



## Kafka中是怎么体现消息顺序性的？

kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。
整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.



## 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1

`offset + 1`



## kafka元数据都有哪些

- **服务器活动状态**
- **Topic和Topic的分区**
- **Partition都有哪些Replication**
- **哪个Replication是Leader**



## kafka producer如何优化打入速度

1. 增加线程

2. 提高 batch.size
3. 增加更多 producer 实例
4. 增加 partition 数
5. 设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解；
6. 跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。



## kafka重要参数

**num.replica.fetchers** ：follower 同步数据的线程数

**batch.size** ：批处理，可用于提高生产者的吞吐量

**linger.ms** ： 批处理超时时间，超过这个时间不管batch装没装满都要发送

**unclean.leader.election.enable** ： 允许非isr集合的机器竞选leader



## kafka  unclean 配置代表啥，会对 spark streaming 消费有什么影响

unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。



## kafka的message格式是什么样的

一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成

header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。

当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，

比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性

body是由N个字节构成的一个消息体，包含了具体的key/value消息



## Kafka中的消息是否会丢失和重复消费？

要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。

1、消息发送

         Kafka消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。Kafka通过配置request.required.acks属性来确认消息的生产：

0---表示不进行消息接收是否成功的确认；
1---表示当Leader接收成功时确认；
-1---表示Leader和Follower都接收成功时确认；
综上所述，有6种消息生产的情况，下面分情况来分析消息丢失的场景：

（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；

（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失；

2、消息消费

Kafka消息消费有两个consumer接口，Low-level API和High-level API：

Low-level API：消费者自己维护offset等值，可以实现对Kafka的完全控制；

High-level API：封装了对parition和offset的管理，使用简单；

如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了；

解决办法：

        针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；
    
        针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。
消息重复消费及解决参考：https://www.javazhiyin.com/22910.html



# 其他



## Session, jwt, oauth2

### 什么是session

>  session 是另一种记录服务器和客户端会话状态的机制

> **session 是基于 cookie 实现的**，session 存储在服务器端，sessionId 会被存储到客户端的cookie 中

![image.png](https://img-blog.csdnimg.cn/img_convert/edd8275bfe8b0a31071aee15b715d8f8.png)

### session 认证流程

1. 用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session
2. 请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器
   浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，**同时 Cookie 记录此 SessionID 属于哪个域名**
3. 当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。

### Cookie 和 Session 的区别

* 安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。
* 存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，需要将其转换成字符串，Session 可以存任意数据类型。
* 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。
* 存储大小不同： 单个 Cookie 保存的数据不能超过 **4K**，Session 可存储数据远高于 Cookie，但是当访问量过多，会占用过多的服务器资源。

### Session的缺点

1. cookie安全性低，容易被CSRF(跨站伪造请求攻击)
2. 分布式部署需要共享session机制
3. 对服务器压力大
4. 可能涉及到数据库查询操作，有潜在的性能问题



### JWT流程（json to web）

1. 用户向服务器发送用户名和密码。 
2. 服务器验证通过后，生成jwt，可以有选择的在其中    保存用户信息及数据。也可以加密。   
3. 服务器向用户返回jwt。 
4. 用户随后的每一次请求，都会在 cookie 或者      header或参数里，将 jwt 传回服务器鉴权。 
5. 服务器收到jwt，找到前期保存的数据，由此得知用户的身份。



### JWT由几部分组成

1. header 

   JWT头部分是一个描述`JWT元数据`的JSON对象，头部包括令牌的类型（即JWT）及使用的哈希算法（如HMAC SHA256或RSA）。

2. Payload

   第二部分是`负载`，内容也是一个JSON对象，通过Base64Url编码，得到一个字符串，这个字符串就是JWT的第二部分(负载)。
   它指定了七个默认字段供选择。

3. Signature

   第三部分是签名哈希部分，是对上面两部分数据签名，**通过指定的算法生成哈希**，以确保数据不会被篡改。首先，需要指定一个密码（secret）。该密码仅仅为保存在服务器中，并且不能向用户公开。然后，使用标头中指定的签名算法（默认情况下为HMAC SHA256）根据以下公式生成签名。



### JWT缺点

1. #### 下列情况，token依然有效

   注销、修改密码、修改用户的权限/橘色、用户账号被删除、用户被注销。

   1. Token一旦被发出，是没有办法销毁的，只能等到它过期。如果想要其失效，可以将token存储在缓存中，服务端每次请求都要验证token是否存在，在上述情况的逻辑里要去Redis删除对应的token，但是这违背了Token无状态的原则。
   2. 为每个用户提供专属的token，要让token失效直接删除token即可。但是这种方式危害极大，分布式情况下每次发送新的token都需要在多台服务器同步。如果用户同时在两个浏览器打开系统，那么一个地方退出登录，其他地方都要重新进行登录。
   3. 让token保持短暂时间，但是这种需要频繁登录。

2. #### token续签问题

   1. 可以每次请求的时候在服务端检查，如果发现快要过期了，就发个新token给浏览器，浏览器每次都要检查新旧token是否一致。不一致，就替换为新的token。但是这个缺点也很大，只有在快要过期的时候那一次请求才会更新token，对客户端也不友好。
   2. 基于第一种，可以每次请求都更新token，但这样开销太大了。
   3. token时间设置长一点，这仅适用于安全性不高的系统。
   4. **服务端返回两个token**，accessToken为真实token(过期时间假设为30min)，refreshToken为过期时间token(过期时间假设为1天)。登录就把这两个token保存在客户端，每次访问传递accessToken，如果客户端检测到accessToken过期，就将refreshToken传给服务端，服务端创建一个新的accessToken返回个客户端保存。其缺点：需要客户端配合，用户注销需要两个token都要失效，重新请求获取token的过程中会有短暂token不可用。



### Token怎么回传给客户端

通过封在http请求的header中



### oauth2.0获得令牌的4种流程

#### 1. 授权码

**授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。**

过程：

1. A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。
2. 用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回`redirect_uri`参数指定的网址。跳转时，会传回一个授权码。
3. A 网站拿到授权码以后，就可以在后端，向 B 网站请求令牌。
4. B 网站收到请求以后，就会颁发令牌。



#### 2. 隐藏式

有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。

过程：

1. A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。
2. 用户跳转到 B 网站，登录后同意给予 A 网站授权。这时，B 网站就会跳回`redirect_uri`参数指定的跳转网址，并且把令牌作为 URL 参数，传给 A 网站。

注意，令牌的位置是 URL 锚点（fragment），而不是查询字符串（querystring），这是因为 OAuth 2.0 允许跳转网址是 HTTP 协议，因此存在"中间人攻击"的风险，而浏览器跳转时，锚点不会发到服务器，就减少了泄漏令牌的风险。



#### 3. 密码式

**如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为"密码式"（password）。**

过程：

1. A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。
2. B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。



#### 4. 凭证式

**最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。**

1. A 应用在命令行向 B 发出请求。
2. B 网站验证通过以后，直接返回令牌。







